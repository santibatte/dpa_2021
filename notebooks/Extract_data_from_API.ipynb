{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE : extract_from_api.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import luigi\n",
    "from datetime import (date, datetime)\n",
    "import pickle\n",
    "import re\n",
    "from sodapy import Socrata\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This should be en utils\"\n",
    "def get_client(token):\n",
    "    return Socrata(\"data.cityofchicago.org\", token)\n",
    "\n",
    "def ingesta_inicial(client, limit=300000):\n",
    "    return client.get(\"4ijn-s7e5\", limit=limit)\n",
    "\n",
    "def ingesta_consecutiva(client, soql_query):\n",
    "    return client.get(\"4ijn-s7e5\", where=soql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "## AWS parameters\n",
    "bucket_name = \"data-product-architecture-equipo-9\"\n",
    "\n",
    "hist_ingest_path = \"ingestion/initial/\"\n",
    "hist_dat_prefix = \"historic-inspections-\"\n",
    "\n",
    "cont_ingest_path = \"ingestion/consecutive/\"\n",
    "cont_dat_prefix = \"consecutive-inspections-\"\n",
    "\n",
    "## Naming files\n",
    "today_info = date.today().strftime('%Y-%m-%d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class APIDataIngestion(luigi.Task):\n",
    "    \n",
    "    ## luigi. parameter... /// se lo envia el task siguiente ... initial consecutive\n",
    "    ingest_type = luigi.Parameter()\n",
    "    \n",
    "    def output(self):\n",
    "        # guardamos en archivo local para que qeude registro de que se ejecuto el task \n",
    "        return luigi.local_target.LocalTarget('/luigi_tmp_files/ingesta_tmp.pkl')\n",
    "    \n",
    "    def run(self):\n",
    "        ## Getting client to download data with API\n",
    "        client = get_client(token)\n",
    "        \n",
    "        if ingest_type=='initial':\n",
    "            ingesta = pickle.dumps(ingesta_inicial(client))\n",
    "         \n",
    "        elif ingest_type=='consecutive':\n",
    "            \n",
    "            ## Finding most recent date in consecutive pickles\n",
    "\n",
    "            #### Getting list with pickles stored in s3 consecutive\n",
    "            objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=cont_ingest_path)['Contents']\n",
    "\n",
    "            #### Regular expression to isolate date in string\n",
    "            regex = str(cont_dat_prefix) + \"(.*).pkl\"\n",
    "\n",
    "            #### List of all dates in consecutive pickles\n",
    "            pkl_dates = [datetime.strptime(re.search(regex, obj[\"Key\"]).group(1), '%Y-%m-%d') for obj in objects if cont_dat_prefix in obj[\"Key\"]]\n",
    "\n",
    "            #### Consecutive pickle most recent date\n",
    "            pkl_mrd = datetime.strftime(max(pkl_dates), '%Y-%m-%d')\n",
    "\n",
    "            ## Building query to download data of interest\n",
    "            soql_query = \"inspection_date >= '{}'\".format(pkl_mrd)\n",
    "\n",
    "            ingesta = pickle.dumps(ingesta_consecutiva(client, soql_query))\n",
    "        \n",
    "        with self.output().open('w') as output_file:\n",
    "            output_file.write(\"test,luigi,s3\")\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luigi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Guardar en S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for this .py \n",
    "import luigi\n",
    "import luigi.contrib.s3\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sansansansan/repos_itam/dpa_2021/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local imports: \n",
    "#from extract_from_api import APIDataIngestion\n",
    "from src.etl.ingesta_almacenamiento import get_s3_resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# existe un bug con bot3 y luigi para pasar las credenciales\n",
    "# necesitas enviar el par√°metro AWS_PROFILE e indicar el profile\n",
    "# con el que quieres que se corra\n",
    "# PYTHONPATH='.' AWS_PROFILE=mge luigi --module ex3_luigi S3Task --local-scheduler ...\n",
    "\n",
    "\n",
    "class S3Task(luigi.Task):\n",
    "    \n",
    "    \n",
    "    ## read '/luigi_tmp_files/ingesta_tmp.pkl'\n",
    "    \n",
    "    \n",
    "    bucket = luigi.Parameter()\n",
    "    root_path = luigi.Parameter()\n",
    "    ingest_type = luigi.Parameter() #initial or consecutive\n",
    "    year = luigi.Parameter()\n",
    "    month = luigi.Parameter()\n",
    "    day=luigi.Parameter()\n",
    "    \n",
    "    def requires(self):\n",
    "        return APIDataIngestion(self)\n",
    "\n",
    "    \n",
    "    def output(self):\n",
    "\n",
    "        output_path = \"s3://{}/{}/{}/{}/YEAR={}/MONTH={}/ingesta.pkl\".\\\n",
    "        format(self.bucket,\n",
    "        self.root_path,\n",
    "        self.ingest_type,\n",
    "        self.year,\n",
    "        str(self.month),\n",
    "        self.day)\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        ## read file... \n",
    "        \n",
    "        s3 = get_s3_resource()\n",
    "        #ses = boto3.session.Session(profile_name='mge', region_name='us-west-2')\n",
    "        #s3_resource = ses.resource('s3')\n",
    "              \n",
    "        ingesta=pickle.load('/luigi_tmp_files/ingesta_tmp.pkl')\n",
    "\n",
    "        with self.output().open('w') as output_file:\n",
    "            output_file.write(\"test,luigi,s3\")\n",
    "            \n",
    "        with self.output().open('w') as s3_f:\n",
    "            joblib.dump(ingesta, s3_f)\n",
    "\n",
    "\n",
    "        return luigi.contrib.s3.S3Target(path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### call from command line\n",
    "bucket = data-product-architecture-equipo-9\n",
    "root_path = ingestion\n",
    "ingest_type = #initial or consecutive\n",
    "year = 2021\n",
    "month = 03\n",
    "#day = 12\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
